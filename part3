<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building Home GPU Clusters - Part 3: Software Stack & Orchestration</title>
    
    <style>
        :root {
            --primary-color: #0c7b93;
            --secondary-color: #00a8cc;
            --accent-color: #ffc947;
            --text-color: #2c3e50;
            --text-light: #6c7b7f;
            --bg-color: #f8fafc;
            --card-bg: #ffffff;
            --border-color: #e2e8f0;
            --success-color: #22c55e;
            --warning-color: #f59e0b;
            --error-color: #ef4444;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 3rem 0;
            text-align: center;
            position: relative;
        }

        .header h1 {
            font-size: 3rem;
            margin-bottom: 1rem;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .header .subtitle {
            font-size: 1.3rem;
            opacity: 0.9;
            margin-bottom: 2rem;
        }

        .part-indicator {
            background: rgba(255,255,255,0.2);
            padding: 0.5rem 1.5rem;
            border-radius: 2rem;
            font-weight: 600;
            margin-bottom: 1rem;
            backdrop-filter: blur(10px);
        }

        .progress-nav {
            background: var(--card-bg);
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 1000;
            padding: 1rem 0;
        }

        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .progress-bar {
            flex: 1;
            height: 8px;
            background: var(--border-color);
            border-radius: 4px;
            margin: 0 2rem;
            overflow: hidden;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            width: 0%;
            transition: width 0.3s ease;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        .section {
            margin: 4rem 0;
        }

        .section-header {
            text-align: center;
            margin-bottom: 3rem;
        }

        .section-header h2 {
            font-size: 2.5rem;
            color: var(--primary-color);
            margin-bottom: 1rem;
            position: relative;
        }

        .section-header h2::after {
            content: '';
            position: absolute;
            bottom: -10px;
            left: 50%;
            transform: translateX(-50%);
            width: 60px;
            height: 4px;
            background: var(--accent-color);
            border-radius: 2px;
        }

        .section-header p {
            font-size: 1.2rem;
            color: var(--text-light);
            max-width: 800px;
            margin: 0 auto;
        }

        .card {
            background: var(--card-bg);
            border-radius: 12px;
            padding: 2rem;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
            border: 1px solid var(--border-color);
            transition: all 0.3s ease;
            margin-bottom: 2rem;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 30px rgba(0,0,0,0.12);
        }

        .card h3 {
            color: var(--primary-color);
            font-size: 1.5rem;
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .card-icon {
            width: 24px;
            height: 24px;
            background: var(--primary-color);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 0.8rem;
        }

        .grid {
            display: grid;
            gap: 2rem;
        }

        .grid-2 {
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
        }

        .grid-3 {
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
        }

        .interactive-demo {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            border-radius: 12px;
            padding: 2rem;
            margin: 2rem 0;
        }

        .demo-content h3 {
            margin-bottom: 1rem;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
        }

        .comparison-table th {
            background: var(--primary-color);
            color: white;
            padding: 1rem;
            font-weight: 600;
            text-align: left;
        }

        .comparison-table td {
            padding: 1rem;
            border-bottom: 1px solid var(--border-color);
            background: var(--card-bg);
        }

        .comparison-table tr:nth-child(even) td {
            background: #f8fafc;
        }

        .spec-highlight {
            font-weight: 600;
            color: var(--primary-color);
        }

        .code-block {
            background: #1a202c;
            color: #e2e8f0;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 1rem 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            position: relative;
        }

        .code-header {
            background: #2d3748;
            color: #e2e8f0;
            padding: 0.5rem 1rem;
            border-radius: 8px 8px 0 0;
            font-size: 0.9rem;
            font-weight: 600;
            margin-bottom: 0;
        }

        .code-block-with-header {
            margin-top: 0;
            border-radius: 0 0 8px 8px;
        }

        .copy-btn {
            position: absolute;
            top: 1rem;
            right: 1rem;
            background: #0c7b93;
            color: white;
            border: none;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.8rem;
        }

        .copy-btn:hover {
            background: #00a8cc;
        }

        .architecture-diagram {
            background: var(--card-bg);
            border-radius: 12px;
            padding: 2rem;
            margin: 2rem 0;
            text-align: center;
        }

        .layer {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1rem;
            margin: 0.5rem;
            border-radius: 8px;
            font-weight: 600;
        }

        .arrow {
            color: var(--primary-color);
            font-size: 2rem;
            margin: 0.5rem 0;
        }

        .performance-metrics {
            background: linear-gradient(135deg, #22c55e 0%, #16a34a 100%);
            color: white;
            border-radius: 12px;
            padding: 2rem;
            margin: 2rem 0;
        }

        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 1rem;
            margin-top: 1rem;
        }

        .metric-item {
            background: rgba(255,255,255,0.1);
            padding: 1rem;
            border-radius: 8px;
            text-align: center;
        }

        .metric-value {
            font-size: 1.5rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .metric-label {
            font-size: 0.9rem;
            opacity: 0.8;
        }

        .info-box {
            padding: 1.5rem;
            border-radius: 8px;
            margin: 1.5rem 0;
            border-left: 4px solid;
        }

        .info-success {
            background: #dcfce7;
            border-color: var(--success-color);
            color: #166534;
        }

        .info-warning {
            background: #fef3c7;
            border-color: var(--warning-color);
            color: #92400e;
        }

        .info-error {
            background: #fee2e2;
            border-color: var(--error-color);
            color: #991b1b;
        }

        footer {
            background: var(--text-color);
            color: white;
            text-align: center;
            padding: 2rem;
            margin-top: 4rem;
        }

        @media (max-width: 768px) {
            .header h1 {
                font-size: 2rem;
            }
            
            .nav-content {
                flex-direction: column;
                gap: 1rem;
            }
            
            .progress-bar {
                margin: 0;
                order: -1;
            }
            
            .container {
                padding: 0 1rem;
            }
            
            .grid-2, .grid-3 {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <header class="header">
        <div style="max-width: 1200px; margin: 0 auto; padding: 0 2rem;">
            <div class="part-indicator">Part 3 of 4</div>
            <h1>Building Home GPU Clusters</h1>
            <p class="subtitle">Software Stack & Orchestration: Container platforms, AI frameworks, monitoring, and optimization</p>
        </div>
    </header>

    <main class="container">
        <section id="overview" class="section">
            <div class="section-header">
                <h2>Software Stack Revolution</h2>
                <p>Modern AI clusters require sophisticated software orchestration to unlock the full potential of distributed GPU infrastructure.</p>
            </div>

            <div class="architecture-diagram">
                <h3>Complete Software Architecture</h3>
                <div class="layer">Applications & AI Frameworks (vLLM, TensorRT-LLM, Dynamo)</div>
                <div class="arrow">↓</div>
                <div class="layer">Container Orchestration (Kubernetes, Docker Swarm)</div>
                <div class="arrow">↓</div>
                <div class="layer">Communication Layer (NCCL, UCX, RDMA)</div>
                <div class="arrow">↓</div>
                <div class="layer">Hardware Abstraction (CUDA, GPU Operator)</div>
                <div class="arrow">↓</div>
                <div class="layer">Infrastructure (GPUs, Network, Storage)</div>
            </div>

            <div class="performance-metrics">
                <h3>🚀 Performance Targets for Home Clusters</h3>
                <div class="metrics-grid">
                    <div class="metric-item">
                        <div class="metric-value">70%+</div>
                        <div class="metric-label">GPU Utilization</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">99%+</div>
                        <div class="metric-label">Cluster Uptime</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">24x</div>
                        <div class="metric-label">Throughput vs Naive</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">1.7x</div>
                        <div class="metric-label">vLLM v1 Speedup</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">30x</div>
                        <div class="metric-label">Dynamo Improvement</div>
                    </div>
                </div>
            </div>
        </section>

        <section id="container-orchestration" class="section">
            <div class="section-header">
                <h2>Container Orchestration Platforms</h2>
                <p>Kubernetes vs Docker Swarm comparison for AI workloads, with practical deployment strategies.</p>
            </div>

            <div class="grid grid-2">
                <div class="card">
                    <h3><div class="card-icon">☸️</div>Kubernetes for AI Clusters</h3>
                    <p><strong>Kubernetes</strong> provides enterprise-grade orchestration with the <span class="spec-highlight">GPU Operator</span> for automated driver management and sophisticated resource scheduling.</p>
                    
                    <div class="info-success">
                        <strong>Key Advantages:</strong>
                        <ul style="margin: 0.5rem 0 0 1rem;">
                            <li>Automatic driver installation and updates</li>
                            <li>Dynamic GPU resource allocation</li>
                            <li>Advanced scheduling policies</li>
                            <li>Built-in service discovery and load balancing</li>
                        </ul>
                    </div>

                    <div class="code-header">Install GPU Operator</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# Add NVIDIA Helm repository
helm repo add nvidia https://nvidia.github.io/gpu-operator
helm repo update

# Install GPU Operator with time-slicing support
helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator \
  --create-namespace \
  --set devicePlugin.config.name=time-slicing-config \
  --set toolkit.enabled=true
                    </div>
                </div>

                <div class="card">
                    <h3><div class="card-icon">🐳</div>Docker Swarm Alternative</h3>
                    <p><strong>Docker Swarm</strong> offers simpler deployment for smaller clusters where ease of management outweighs advanced features.</p>
                    
                    <div class="info-warning">
                        <strong>Trade-offs:</strong>
                        <ul style="margin: 0.5rem 0 0 1rem;">
                            <li>Manual GPU driver configuration required</li>
                            <li>Limited resource scheduling capabilities</li>
                            <li>Better suited for dedicated single-application clusters</li>
                            <li>Simpler networking model</li>
                        </ul>
                    </div>

                    <div class="code-header">Docker Swarm GPU Setup</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# Initialize swarm on manager node
docker swarm init --advertise-addr MANAGER-IP

# Add GPU runtime to daemon.json
{
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  },
  "default-runtime": "nvidia"
}

# Deploy GPU service with resource constraints
docker service create \
  --generic-resource "NVIDIA-GPU=2" \
  --replicas 1 \
  my-ai-app:latest
                    </div>
                </div>
            </div>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Kubernetes</th>
                        <th>Docker Swarm</th>
                        <th>Home Cluster Recommendation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Setup Complexity</strong></td>
                        <td class="spec-highlight">High (2-5 days)</td>
                        <td>Low (hours)</td>
                        <td>K8s for 4+ nodes</td>
                    </tr>
                    <tr>
                        <td><strong>GPU Management</strong></td>
                        <td class="spec-highlight">Automated (GPU Operator)</td>
                        <td>Manual configuration</td>
                        <td>K8s strongly preferred</td>
                    </tr>
                    <tr>
                        <td><strong>Resource Utilization</strong></td>
                        <td class="spec-highlight">Excellent (time-slicing)</td>
                        <td>Good</td>
                        <td>K8s for multi-tenant</td>
                    </tr>
                    <tr>
                        <td><strong>Scaling</strong></td>
                        <td class="spec-highlight">Unlimited</td>
                        <td>Limited</td>
                        <td>K8s for growth plans</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="ai-frameworks" class="section">
            <div class="section-header">
                <h2>AI Framework Deployment</h2>
                <p>Comprehensive comparison and deployment strategies for vLLM, TensorRT-LLM, and emerging frameworks.</p>
            </div>

            <div class="grid grid-3">
                <div class="card">
                    <h3><div class="card-icon">🚀</div>vLLM - The Performance Leader</h3>
                    <p><strong>vLLM v1 Alpha</strong> delivers <span class="spec-highlight">1.7x speedup</span> over previous versions with PagedAttention reducing memory waste from 70% to under 4%.</p>
                    
                    <div style="margin: 1rem 0;">
                        <strong>Key Features:</strong>
                        <ul style="margin-left: 1rem;">
                            <li>Continuous batching for 10x+ throughput</li>
                            <li>Multi-node tensor parallelism</li>
                            <li>Speculative decoding (2-3x acceleration)</li>
                            <li>Ray cluster integration</li>
                        </ul>
                    </div>

                    <div class="code-header">vLLM Multi-Node Deployment</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# Launch Ray cluster head
ray start --head --port=6379 \
  --dashboard-host=0.0.0.0

# Worker nodes join cluster
ray start --address=head-node:6379

# Start vLLM with tensor parallelism
python -m vllm.entrypoints.api_server \
  --model meta-llama/Llama-2-70b-hf \
  --tensor-parallel-size 4 \
  --pipeline-parallel-size 2 \
  --host 0.0.0.0 --port 8000
                    </div>
                </div>

                <div class="card">
                    <h3><div class="card-icon">⚡</div>TensorRT-LLM - Raw Speed</h3>
                    <p><strong>TensorRT-LLM</strong> achieves <span class="spec-highlight">650-700 tokens/second</span> on Llama 70B models but requires model compilation and complex setup.</p>
                    
                    <div style="margin: 1rem 0;">
                        <strong>Performance Benefits:</strong>
                        <ul style="margin-left: 1rem;">
                            <li>5-10% higher throughput than vLLM</li>
                            <li>Optimized CUDA kernels</li>
                            <li>Advanced quantization support</li>
                            <li>Native TensorRT integration</li>
                        </ul>
                    </div>

                    <div class="code-header">TensorRT-LLM Build Process</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# Convert HuggingFace model to TensorRT
python convert_checkpoint.py \
  --model_dir meta-llama/Llama-2-70b-hf \
  --output_dir llama_70b_ckpt \
  --dtype float16 \
  --tp_size 4

# Build optimized engine
trtllm-build \
  --checkpoint_dir llama_70b_ckpt \
  --output_dir llama_70b_engine \
  --gemm_plugin float16 \
  --max_batch_size 64
                    </div>
                </div>

                <div class="card">
                    <h3><div class="card-icon">🔥</div>NVIDIA Dynamo - Next Gen</h3>
                    <p><strong>Dynamo Framework</strong> promises <span class="spec-highlight">30x throughput improvements</span> for reasoning models on Blackwell GPUs with Rust-based performance.</p>
                    
                    <div style="margin: 1rem 0;">
                        <strong>Revolutionary Features:</strong>
                        <ul style="margin-left: 1rem;">
                            <li>Built in Rust for maximum performance</li>
                            <li>Optimized for DeepSeek-R1 reasoning</li>
                            <li>Blackwell GPU acceleration</li>
                            <li>Python extensibility layer</li>
                        </ul>
                    </div>

                    <div class="info-warning">
                        <strong>Status:</strong> Early preview for Blackwell architecture. Limited availability for home clusters until H100/RTX 5000 series adoption.
                    </div>
                </div>
            </div>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Framework</th>
                        <th>Setup Complexity</th>
                        <th>Performance</th>
                        <th>Multi-Node Support</th>
                        <th>Best Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>vLLM</strong></td>
                        <td class="spec-highlight">Simple (pip install)</td>
                        <td>Excellent (1.7x v1 improvement)</td>
                        <td class="spec-highlight">Native Ray integration</td>
                        <td>General inference clusters</td>
                    </tr>
                    <tr>
                        <td><strong>TensorRT-LLM</strong></td>
                        <td>Complex (compilation required)</td>
                        <td class="spec-highlight">Best raw performance</td>
                        <td>Manual multi-node setup</td>
                        <td>Maximum performance scenarios</td>
                    </tr>
                    <tr>
                        <td><strong>Text Generation Inference</strong></td>
                        <td>Medium (Docker-based)</td>
                        <td>Good (optimized for HuggingFace)</td>
                        <td>Limited</td>
                        <td>HuggingFace model hosting</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="monitoring-systems" class="section">
            <div class="section-header">
                <h2>Monitoring & Observability</h2>
                <p>Comprehensive monitoring stack for tracking GPU utilization, thermal performance, and application metrics.</p>
            </div>

            <div class="grid grid-2">
                <div class="card">
                    <h3><div class="card-icon">📊</div>NVIDIA DCGM Integration</h3>
                    <p><strong>Data Center GPU Manager (DCGM)</strong> provides comprehensive GPU monitoring with Prometheus integration for time-series analysis.</p>
                    
                    <div class="code-header">DCGM Prometheus Setup</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# Install DCGM exporter in Kubernetes
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/dcgm-exporter/main/dcgm-exporter.yaml

# Configure Prometheus to scrape DCGM metrics
scrape_configs:
- job_name: 'dcgm'
  static_configs:
  - targets: ['dcgm-exporter:9400']
  scrape_interval: 5s
  metrics_path: /metrics

# Key metrics to monitor:
# - DCGM_FI_DEV_GPU_UTIL (GPU utilization %)
# - DCGM_FI_DEV_MEM_COPY_UTIL (memory bandwidth %)
# - DCGM_FI_DEV_GPU_TEMP (temperature °C)
# - DCGM_FI_DEV_POWER_USAGE (power consumption W)
                    </div>
                </div>

                <div class="card">
                    <h3><div class="card-icon">📈</div>Grafana Dashboard Design</h3>
                    <p><strong>Grafana</strong> visualization with custom dashboards for GPU cluster monitoring, alerting on thermal limits and utilization thresholds.</p>
                    
                    <div style="margin: 1rem 0;">
                        <strong>Essential Dashboard Panels:</strong>
                        <ul style="margin-left: 1rem;">
                            <li>Real-time GPU utilization heatmap</li>
                            <li>Memory bandwidth utilization trends</li>
                            <li>Temperature monitoring with alerts</li>
                            <li>Power consumption per GPU/node</li>
                            <li>Model inference latency metrics</li>
                        </ul>
                    </div>

                    <div class="code-header">Grafana Alert Rules</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# High temperature alert (>85°C)
- alert: GPUHighTemperature
  expr: DCGM_FI_DEV_GPU_TEMP > 85
  for: 2m
  labels:
    severity: warning
  annotations:
    summary: "GPU temperature critical"
    
# Low utilization alert (<30% for 10min)
- alert: GPULowUtilization
  expr: avg_over_time(DCGM_FI_DEV_GPU_UTIL[10m]) < 30
  for: 10m
  labels:
    severity: info
  annotations:
    summary: "GPU underutilized"
                    </div>
                </div>
            </div>

            <div class="performance-metrics">
                <h3>📋 Key Performance Indicators (KPIs)</h3>
                <p>Target metrics for optimal home cluster operation:</p>
                
                <div class="metrics-grid">
                    <div class="metric-item">
                        <div class="metric-value">70%+</div>
                        <div class="metric-label">Average GPU Utilization</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">85°C</div>
                        <div class="metric-label">Max GPU Temperature</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">90%+</div>
                        <div class="metric-label">Memory Bandwidth Usage</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">99.5%+</div>
                        <div class="metric-label">Cluster Uptime</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value"><100ms</div>
                        <div class="metric-label">Inter-node Latency</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">24x</div>
                        <div class="metric-label">PagedAttention Improvement</div>
                    </div>
                </div>
            </div>
        </section>

        <section id="performance-optimization" class="section">
            <div class="section-header">
                <h2>Performance Optimization Strategies</h2>
                <p>Advanced techniques for maximizing GPU utilization, reducing latency, and scaling inference throughput.</p>
            </div>

            <div class="grid grid-2">
                <div class="card">
                    <h3><div class="card-icon">⚡</div>Memory Optimization Techniques</h3>
                    <p><strong>PagedAttention</strong> revolutionizes memory management by implementing non-contiguous block allocation, reducing waste from 70% to under 4%.</p>
                    
                    <div style="margin: 1rem 0;">
                        <strong>Advanced Memory Strategies:</strong>
                        <ul style="margin-left: 1rem;">
                            <li><span class="spec-highlight">KV-cache sharding</span> across multiple GPUs</li>
                            <li>Dynamic memory allocation for variable-length sequences</li>
                            <li>Intelligent prefetching for pipeline parallelism</li>
                            <li>Memory pool optimization for batch processing</li>
                        </ul>
                    </div>

                    <div class="code-header">vLLM Memory Configuration</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# Optimize memory usage for large models
python -m vllm.entrypoints.api_server \
  --model meta-llama/Llama-2-70b-hf \
  --gpu-memory-utilization 0.95 \
  --max-model-len 4096 \
  --block-size 16 \
  --swap-space 4 \
  --cpu-offload-gb 8 \
  --enable-chunked-prefill
                    </div>
                </div>

                <div class="card">
                    <h3><div class="card-icon">🔄</div>Batching & Parallelism</h3>
                    <p><strong>Continuous batching</strong> enables 10x+ throughput improvements by dynamically managing request queues and optimizing GPU occupancy.</p>
                    
                    <div style="margin: 1rem 0;">
                        <strong>Parallelism Strategies:</strong>
                        <ul style="margin-left: 1rem;">
                            <li><span class="spec-highlight">Tensor parallelism</span> for model weights distribution</li>
                            <li>Pipeline parallelism for sequential processing</li>
                            <li>Data parallelism for batch processing</li>
                            <li>Hybrid approaches for maximum efficiency</li>
                        </ul>
                    </div>

                    <div class="code-header">Optimal Batching Configuration</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# Configure for maximum throughput
export VLLM_ATTENTION_BACKEND=FLASHINFER
export VLLM_USE_TRITON_FLASH_ATTENTION=1

# Launch with optimized batch sizes
--max-num-batched-tokens 8192 \
--max-num-seqs 256 \
--tensor-parallel-size 4 \
--pipeline-parallel-size 2 \
--enable-prefix-caching \
--disable-log-stats
                    </div>
                </div>
            </div>

            <div class="card">
                <h3><div class="card-icon">🎯</div>Quantization and Compression</h3>
                <p><strong>Advanced quantization techniques</strong> enable running larger models on limited hardware while preserving quality.</p>
                
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Quantization Method</th>
                            <th>Memory Reduction</th>
                            <th>Quality Impact</th>
                            <th>Speed Impact</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>AWQ (Activation-aware)</strong></td>
                            <td class="spec-highlight">4x reduction</td>
                            <td>Minimal (~1% loss)</td>
                            <td class="spec-highlight">1.2-1.5x faster</td>
                            <td>Production inference</td>
                        </tr>
                        <tr>
                            <td><strong>GPTQ</strong></td>
                            <td class="spec-highlight">4x reduction</td>
                            <td>Low (~2% loss)</td>
                            <td>1.1-1.3x faster</td>
                            <td>General purpose</td>
                        </tr>
                        <tr>
                            <td><strong>SmoothQuant</strong></td>
                            <td>4x reduction</td>
                            <td>Very low (~0.5% loss)</td>
                            <td>1.3-1.7x faster</td>
                            <td>High accuracy needs</td>
                        </tr>
                        <tr>
                            <td><strong>GGUF/GGML</strong></td>
                            <td class="spec-highlight">2-8x reduction</td>
                            <td>Variable</td>
                            <td>CPU+GPU hybrid</td>
                            <td>Consumer hardware</td>
                        </tr>
                    </tbody>
                </table>

                <div class="code-header">AWQ Quantization Example</div>
                <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# Install AutoAWQ for quantization
pip install autoawq

# Quantize model for deployment
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_path = "meta-llama/Llama-2-70b-hf"
quant_path = "llama-70b-awq"

# Load and quantize
model = AutoAWQForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Quantize with calibration data
model.quantize(tokenizer, quant_config={"zero_point": True, "q_group_size": 128})
model.save_quantized(quant_path)

# Deploy with vLLM
python -m vllm.entrypoints.api_server \
  --model ./llama-70b-awq \
  --quantization awq
                </div>
            </div>
        </section>

        <section id="deployment-automation" class="section">
            <div class="section-header">
                <h2>Deployment Automation</h2>
                <p>Infrastructure as Code (IaC) approaches using Helm charts, Ansible playbooks, and GitOps workflows.</p>
            </div>

            <div class="grid grid-2">
                <div class="card">
                    <h3><div class="card-icon">📦</div>Helm Chart Template</h3>
                    <p><strong>Helm charts</strong> provide templated deployments for AI workloads with configurable resource allocation and scaling policies.</p>
                    
                    <div class="code-header">AI Workload Helm Chart</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# values.yaml for vLLM deployment
image:
  repository: vllm/vllm-openai
  tag: latest
  pullPolicy: IfNotPresent

model:
  name: meta-llama/Llama-2-70b-hf
  tensorParallelSize: 4
  maxModelLen: 4096
  
resources:
  limits:
    nvidia.com/gpu: 4
    memory: 128Gi
    cpu: 16
  requests:
    nvidia.com/gpu: 4
    memory: 96Gi
    cpu: 12

autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 3
  targetGPUUtilization: 80

service:
  type: LoadBalancer
  port: 8000
                    </div>
                </div>

                <div class="card">
                    <h3><div class="card-icon">🤖</div>Ansible Automation</h3>
                    <p><strong>Ansible playbooks</strong> automate cluster provisioning, software installation, and configuration management across all nodes.</p>
                    
                    <div class="code-header">Cluster Provisioning Playbook</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
---
- name: GPU Cluster Setup
  hosts: gpu_nodes
  become: yes
  
  tasks:
  - name: Install NVIDIA drivers
    package:
      name: nvidia-driver-535
      state: present
    
  - name: Install Docker
    shell: |
      curl -fsSL https://get.docker.com | sh
      usermod -aG docker $USER
      
  - name: Install NVIDIA Container Runtime
    package:
      name: nvidia-container-runtime
      state: present
      
  - name: Configure Docker daemon
    copy:
      content: |
        {
          "default-runtime": "nvidia",
          "runtimes": {
            "nvidia": {
              "path": "nvidia-container-runtime",
              "runtimeArgs": []
            }
          }
        }
      dest: /etc/docker/daemon.json
    notify: restart docker
    
  - name: Install Kubernetes
    shell: |
      curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
      add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
      apt-get update && apt-get install -y kubelet kubeadm kubectl
                    </div>
                </div>
            </div>

            <div class="info-success">
                <strong>GitOps Benefits:</strong> Declarative configuration management ensures your cluster state matches your Git repository, enabling easy rollbacks, audit trails, and collaborative changes.
            </div>
        </section>

        <section id="troubleshooting" class="section">
            <div class="section-header">
                <h2>Troubleshooting & Best Practices</h2>
                <p>Common issues, diagnostic techniques, and security considerations for production GPU clusters.</p>
            </div>

            <div class="grid grid-2">
                <div class="card">
                    <h3><div class="card-icon">🔧</div>Common GPU Issues</h3>
                    
                    <div class="info-error">
                        <strong>Out of Memory (OOM) Errors:</strong><br>
                        Solution: Reduce batch size, enable CPU offloading, implement model sharding
                    </div>
                    
                    <div class="info-warning">
                        <strong>Low GPU Utilization (&lt;50%):</strong><br>
                        Solution: Increase batch size, optimize data loading, check CPU bottlenecks
                    </div>
                    
                    <div class="info-error">
                        <strong>NCCL Initialization Failures:</strong><br>
                        Solution: Verify network connectivity, check firewall settings, ensure consistent CUDA versions
                    </div>

                    <div class="code-header">GPU Diagnostic Commands</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# Check GPU status
nvidia-smi

# Verify CUDA installation
nvcc --version

# Test GPU accessibility from containers
docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi

# Check NCCL functionality
python -c "import torch; print(torch.cuda.nccl.version())"

# Verify multi-GPU communication
python -c "
import torch
print(f'GPUs available: {torch.cuda.device_count()}')
if torch.cuda.device_count() > 1:
    for i in range(torch.cuda.device_count()):
        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
"
                    </div>
                </div>

                <div class="card">
                    <h3><div class="card-icon">🛡️</div>Security Best Practices</h3>
                    
                    <div style="margin: 1rem 0;">
                        <strong>Network Security:</strong>
                        <ul style="margin-left: 1rem;">
                            <li>VPN access for remote management</li>
                            <li>Firewall rules for GPU cluster traffic</li>
                            <li>TLS encryption for API endpoints</li>
                            <li>Network segmentation for isolation</li>
                        </ul>
                    </div>

                    <div style="margin: 1rem 0;">
                        <strong>Container Security:</strong>
                        <ul style="margin-left: 1rem;">
                            <li>Non-root container execution</li>
                            <li>Resource limits and quotas</li>
                            <li>Image vulnerability scanning</li>
                            <li>Secrets management</li>
                        </ul>
                    </div>

                    <div class="code-header">Security Configuration</div>
                    <div class="code-block code-block-with-header">
<button class="copy-btn" onclick="copyCode(this)">Copy</button>
# Pod Security Policy for GPU workloads
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: gpu-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
                    </div>
                </div>
            </div>
        </section>

        <section id="summary" class="section">
            <div class="section-header">
                <h2>Part 3 Complete: Software Mastery Achieved</h2>
                <p>You now have the complete software stack knowledge to deploy and operate a professional-grade home GPU cluster.</p>
            </div>

            <div class="grid grid-2">
                <div class="card">
                    <h3><div class="card-icon">✅</div>What You've Mastered</h3>
                    <ul style="margin-left: 1rem;">
                        <li><strong>Container orchestration</strong> with Kubernetes and Docker</li>
                        <li><strong>AI framework deployment</strong> (vLLM, TensorRT-LLM)</li>
                        <li><strong>Performance optimization</strong> techniques</li>
                        <li><strong>Monitoring and observability</strong> systems</li>
                        <li><strong>Deployment automation</strong> with GitOps</li>
                        <li><strong>Security best practices</strong> for production</li>
                    </ul>
                </div>

                <div class="card">
                    <h3><div class="card-icon">🎯</div>Key Achievements</h3>
                    <ul style="margin-left: 1rem;">
                        <li><span class="spec-highlight">70%+ GPU utilization</span> through optimized batching</li>
                        <li><span class="spec-highlight">24x throughput improvement</span> with PagedAttention</li>
                        <li><span class="spec-highlight">99.5%+ uptime</span> with proper monitoring</li>
                        <li><span class="spec-highlight">4x memory efficiency</span> through quantization</li>
                        <li><span class="spec-highlight">Sub-100ms latency</span> for distributed inference</li>
                        <li><span class="spec-highlight">Automated deployment</span> with zero-downtime updates</li>
                    </ul>
                </div>
            </div>

            <div class="performance-metrics">
                <h3>🏆 Your Cluster Performance Profile</h3>
                <p>With proper implementation of Part 3 techniques, your home cluster now matches enterprise capabilities:</p>
                
                <div class="metrics-grid">
                    <div class="metric-item">
                        <div class="metric-value">Enterprise</div>
                        <div class="metric-label">Grade Monitoring</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">Production</div>
                        <div class="metric-label">Ready Deployment</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">Auto</div>
                        <div class="metric-label">Scaling Capable</div>
                    </div>
                    <div class="metric-item">
                        <div class="metric-value">Cloud</div>
                        <div class="metric-label">Native Architecture</div>
                    </div>
                </div>
            </div>

            <div class="interactive-demo">
                <div class="demo-content">
                    <h3>🚀 Coming Next: Part 4 - Advanced Applications</h3>
                    <p>With our software stack deployed, Part 4 explores advanced AI applications and real-world use cases:</p>
                    
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; margin: 1rem 0;">
                        <div style="background: rgba(255,255,255,0.1); padding: 1rem; border-radius: 8px; text-align: center;">
                            <h4>🧠 Large Language Models</h4>
                            <div>Running Llama 405B at home</div>
                        </div>
                        <div style="background: rgba(255,255,255,0.1); padding: 1rem; border-radius: 8px; text-align: center;">
                            <h4>🎨 Creative AI</h4>
                            <div>Stable Diffusion clusters</div>
                        </div>
                        <div style="background: rgba(255,255,255,0.1); padding: 1rem; border-radius: 8px; text-align: center;">
                            <h4>⚗️ Research</h4>
                            <div>Custom model experiments</div>
                        </div>
                        <div style="background: rgba(255,255,255,0.1); padding: 1rem; border-radius: 8px; text-align: center;">
                            <h4>🏭 Production</h4>
                            <div>API gateways & scaling</div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <div style="text-align: center; padding: 3rem 0; background: var(--bg-color);">
        <a href="part4.html" style="display: inline-block; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 1rem 2rem; border-radius: 8px; text-decoration: none; font-weight: 600; font-size: 1.1rem; transition: all 0.3s ease; box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);">
            Continue to Part 4: Advanced Applications & Use Cases →
        </a>
        <p style="margin-top: 1rem; color: var(--text-light); font-size: 0.9rem;">
            Deploy massive language models, creative AI, and production systems
        </p>
    </div>

    <footer>
        <div style="max-width: 1200px; margin: 0 auto; padding: 0 2rem;">
            <p style="font-size: 1.1rem; margin-bottom: 0.5rem;">Part 3 of 4: Building Home GPU Clusters | <strong>Software Stack & Orchestration Complete</strong></p>
            <p style="opacity: 0.7; margin-bottom: 1rem;">Next: Advanced Applications, Large Models, and Production Use Cases</p>
        </div>
    </footer>

    <script>
        function copyCode(button) {
            var codeBlock = button.parentNode;
            var code = codeBlock.textContent.replace(button.textContent, '').trim();
            
            navigator.clipboard.writeText(code).then(function() {
                var originalText = button.textContent;
                button.textContent = 'Copied!';
                button.style.background = '#22c55e';
                
                setTimeout(function() {
                    button.textContent = originalText;
                    button.style.background = '#0c7b93';
                }, 2000);
            }).catch(function(err) {
                console.error('Failed to copy code: ', err);
                button.textContent = 'Copy failed';
                setTimeout(function() {
                    button.textContent = 'Copy';
                }, 2000);
            });
        }
    </script>
</body>
</html></p
